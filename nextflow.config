/*
 * RComPlEx Pipeline Configuration
 * ================================
 * Nextflow configuration for SLURM + Apptainer container execution
 * All software (R, tidyverse, igraph, etc.) comes from the SIF container
 */

// Manifest
manifest {
    name = 'RComPlEx'
    author = 'Torgeir RhodÃ©n Hvidsten & Martin Paliocha'
    description = 'Co-expressolog discovery across annual and perennial species'
    version = '1.0.0'
    nextflowVersion = '>=21.04.0'
    defaultBranch = 'main'
}

// Global default parameters
params {
    config = "${projectDir}/config/pipeline_config.yaml"
    workdir = "${projectDir}"
    tissues = ['root', 'leaf']
    outdir = "${projectDir}/results"
    test_mode = false
    help = false
}

// Container configuration (enabled by default)
singularity {
    enabled = true
    autoMounts = true
    cacheDir = "${projectDir}/.singularity"
}

// Disable unused container runtimes
conda {
    enabled = false
}

docker {
    enabled = false
}

// Execution profiles
profiles {

    // Default: SLURM + Apptainer container (all software from SIF)
    slurm {
        process {
            executor = 'slurm'
            queue = 'orion'
            clusterOptions = '--qos=normal'

            // Load Apptainer on compute nodes
            beforeScript = 'module load singularity'

            // Use container for all processes
            container = "${projectDir}/RComPlEx.sif"

            // Per-process resource requirements
            withName: PREPARE_PAIR {
                cpus = 2
                memory = '8 GB'
                time = '15m'
                maxForks = 20
            }

            withName: RCOMPLEX_01_LOAD_FILTER {
                cpus = 2
                memory = '8 GB'
                time = '15m'
                maxForks = 20
            }

            withName: RCOMPLEX_02_COMPUTE_NETWORKS {
                cpus = 24
                memory = '280 GB'
                time = '4h'
                maxForks = 10
            }

            withName: RCOMPLEX_03_NETWORK_COMPARISON {
                cpus = 24
                memory = '280 GB'
                time = '4h'
                errorStrategy = { task.attempt <= 2 ? 'retry' : 'ignore' }
                maxRetries = 2
                maxForks = 3
            }

            withName: RCOMPLEX_04_SUMMARY_STATS {
                cpus = 4
                memory = '16 GB'
                time = '1h'
                maxForks = 10
            }

            withName: FIND_CLIQUES {
                cpus = 12
                memory = '220 GB'
                time = '2d'
            }
        }

        executor {
            name = 'slurm'
            queueSize = 100
            submitRateLimit = '10 sec'
            pollInterval = '30 sec'
        }
    }

    // Local development (uses container)
    standard {
        process {
            executor = 'local'
            container = "${projectDir}/RComPlEx.sif"
            cpus = 2
            memory = '8 GB'
        }
    }

    // Test profile: limited pairs for quick testing
    test {
        params.test_mode = true
        params.tissues = ['root']

        process {
            executor = 'slurm'
            queue = 'orion'
            beforeScript = 'module load singularity'
            container = "${projectDir}/RComPlEx.sif"

            withName: PREPARE_PAIR {
                maxForks = 2
            }
        }
    }

    // Debug profile: verbose output
    debug {
        process {
            beforeScript = 'echo "Task: $task.name"; echo "Work dir: $task.workDir"; module load singularity'
        }
        cleanup = false
    }
}

// Execution report options
timeline {
    enabled = true
    file = "${params.outdir}/timeline.html"
    overwrite = true
}

report {
    enabled = true
    file = "${params.outdir}/report.html"
    overwrite = true
}

trace {
    enabled = true
    file = "${params.outdir}/trace.txt"
    overwrite = true
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,submit,start,complete,duration,realtime,cpus,memory,rss,vmem,peak_rss,peak_vmem'
}

dag {
    enabled = true
    file = "${params.outdir}/dag.svg"
    overwrite = true
}

// Logging
log {
    dateFormat = 'yyyy-MM-dd HH:mm:ss'
}

// Work directory cleanup
cleanup = false  // Set to true to remove work directories after completion

// Resource limits
process {
    // Enable aggressive caching for smart restarts
    cache = 'lenient'

    // Default error strategy
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries = 1

    // Default resource limits
    cpus = { check_max( 2, 'cpus' ) }
    memory = { check_max( 8.GB * task.attempt, 'memory' ) }
    time = { check_max( 4.h * task.attempt, 'time' ) }
}

// Function to ensure that resource requirements don't go beyond maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

// Maximum resource defaults
params.max_memory = 500.GB
params.max_cpus = 48
params.max_time = 30.d
