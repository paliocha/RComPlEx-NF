/*
 * RComPlEx Pipeline Configuration
 * ================================
 * Nextflow configuration for SLURM + Apptainer container execution
 * All software (R, tidyverse, igraph, etc.) comes from the SIF container
 */

// Manifest
manifest {
    name = 'RComPlEx'
    author = 'Torgeir RhodÃ©n Hvidsten & Martin Paliocha'
    description = 'Co-expressolog discovery across annual and perennial species'
    version = '1.0.0'
    nextflowVersion = '>=21.04.0'
    defaultBranch = 'main'
}

// Profile metadata (for documentation)
params {
    config_profile_description = 'RComPlEx pipeline optimized for NMBU Orion HPC cluster.'
    config_profile_contact = 'Martin Paliocha (martin.paliocha@nmbu.no)'
    config_profile_url = 'https://orion.nmbu.no/'
}

// Global default parameters
params {
    config = "${projectDir}/config/pipeline_config.yaml"
    workdir = "${projectDir}"
    tissues = ['root', 'leaf']
    outdir = "${projectDir}/results"
    test_mode = false
    help = false
}

// Container configuration (enabled by default)
singularity {
    enabled = true
    autoMounts = true
    cacheDir = '/local/genome/packages/singularity/CACHE/nextflow'
    runOptions = "--bind $TMPDIR:/tmp"
}

// Disable unused container runtimes
conda {
    enabled = false
}

docker {
    enabled = false
}

// Execution profiles
profiles {

    // Default: SLURM + Apptainer container (all software from SIF)
    slurm {
        process {
            executor = 'slurm'
            queue = 'orion'
            clusterOptions = '--qos=normal'
            scratch = true  // Use local scratch for I/O performance

            // Load Apptainer on compute nodes
            beforeScript = 'module load singularity'

            // Use container for all processes
            container = "${projectDir}/RComPlEx.sif"

            // Resource allocation by labels
            withLabel: 'low_mem' {
                cpus = 2
                memory = '8 GB'
                time = '30m'
                maxForks = 20
            }
            
            withLabel: 'medium_mem' {
                cpus = 4
                memory = '16 GB'
                time = '2h'
                maxForks = 10
            }
            
            withLabel: 'high_mem' {
                cpus = 2
                memory = '500 GB'
                time = '24h'
                maxForks = 5
            }
            
            withLabel: 'very_high_mem' {
                cpus = 12
                memory = '280 GB'
                time = '2d'
                maxForks = 2
            }

            // Per-process resource requirements
            withName: PREPARE_PAIR {
                cpus = 2
                memory = '8 GB'
                time = '15m'
                maxForks = 20
            }

            withName: RCOMPLEX_01_LOAD_FILTER {
                cpus = 2
                memory = '8 GB'
                time = '15m'
                maxForks = 20
            }

            withName: RCOMPLEX_02_COMPUTE_NETWORKS {
                cpus = { task.attempt == 1 ? 8 : 24 }
                memory = { task.attempt == 1 ? '600 GB' : '800 GB' }
                time = '36h'
                maxForks = 4
                errorStrategy = { task.exitStatus == 137 ? 'retry' : 'terminate' }
                maxRetries = 1
            }

            withName: RCOMPLEX_03_NETWORK_COMPARISON {
                cpus = 4
                memory = { task.attempt == 1 ? '150 GB' : '300 GB' }
                time = '36h'
                maxForks = 5
                errorStrategy = { task.exitStatus == 137 ? 'retry' : 'terminate' }
                maxRetries = 1
            }

            withName: RCOMPLEX_04_SUMMARY_STATS {
                cpus = 4
                memory = '16 GB'
                time = '24h'
                maxForks = 10
            }

            withName: FIND_CLIQUES {
                cpus = 12
                memory = '280 GB'
                time = '2d'
            }
        }

        executor {
            name = 'slurm'
            queueSize = 100
            submitRateLimit = '30/1min'  // Prevent queue saturation (tested limit on Orion)
            pollInterval = '30 sec'
        }
    }

    // Local development (uses container)
    standard {
        process {
            executor = 'local'
            container = "${projectDir}/RComPlEx.sif"
            cpus = 2
            memory = '8 GB'
        }
    }

    // Test profile: limited pairs for quick testing
    test {
        params.test_mode = true
        params.tissues = ['root']

        process {
            executor = 'slurm'
            queue = 'orion'
            beforeScript = 'module load singularity'
            container = "${projectDir}/RComPlEx.sif"

            withName: PREPARE_PAIR {
                maxForks = 2
            }
        }
    }

    // Debug profile: verbose output
    debug {
        process {
            beforeScript = 'echo "Task: $task.name"; echo "Work dir: $task.workDir"; module load singularity'
        }
        cleanup = false
    }
}

// Execution report options
timeline {
    enabled = true
    file = "${params.outdir}/timeline.html"
    overwrite = true
}

report {
    enabled = true
    file = "${params.outdir}/report.html"
    overwrite = true
}

trace {
    enabled = true
    file = "${params.outdir}/trace.txt"
    overwrite = true
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,submit,start,complete,duration,realtime,cpus,memory,rss,vmem,peak_rss,peak_vmem'
}

dag {
    enabled = true
    file = "${params.outdir}/dag.svg"
    overwrite = true
}

// Logging
log {
    dateFormat = 'yyyy-MM-dd HH:mm:ss'
}

// Work directory cleanup
cleanup = false  // Set to true to remove work directories after completion

// Resource limits
process {
    // Enable aggressive caching for smart restarts
    cache = 'lenient'

    // Default error strategy (retry OOM and common transient errors)
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries = 1
    
    // Retry with more memory on OOM
    memory = { task.exitStatus == 137 && task.attempt > 1 ? check_max( 8.GB * task.attempt * 1.5, 'memory' ) : check_max( 8.GB * task.attempt, 'memory' ) }

    // Default resource limits
    cpus = { check_max( 2, 'cpus' ) }
    memory = { check_max( 8.GB * task.attempt, 'memory' ) }
    time = { check_max( 36.h * task.attempt, 'time' ) }
}

// Function to ensure that resource requirements don't go beyond maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

// Maximum resource defaults
params.max_memory = 800.GB
params.max_cpus = 48
params.max_time = 30.d
