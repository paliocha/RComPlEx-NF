/*
 * RComPlEx Pipeline Configuration
 * ================================
 * Nextflow configuration for SLURM + Apptainer container execution
 * All software (R, tidyverse, igraph, etc.) comes from the SIF container
 */

// Manifest
manifest {
    name = 'RComPlEx'
    author = 'Torgeir RhodÃ©n Hvidsten & Martin Paliocha'
    description = 'Co-expressolog discovery across annual and perennial species'
    version = '1.0.0'
    nextflowVersion = '>=21.04.0'
    defaultBranch = 'main'
}

// Profile metadata (for documentation)
params {
    config_profile_description = 'RComPlEx pipeline optimized for NMBU Orion HPC cluster.'
    config_profile_contact = 'Martin Paliocha (martin.paliocha@nmbu.no)'
    config_profile_url = 'https://orion.nmbu.no/'
}

// Global default parameters
params {
    // Pipeline configuration - located with scripts in $HOME
    config = "${System.getenv('HOME')}/AnnualPerennial/RComPlEx/config/pipeline_config.yaml"
    
    // Directory configuration - easily override with --workdir and --outdir
    // Uses environment variables: $HOME (scripts) and $PROJECTS (data)
    // Scripts/config in $HOME (small storage), data/output in $PROJECTS (large storage)
    workdir = "${System.getenv('PROJECTS')}/FjellheimLab/martpali/AnnualPerennial/RComPlEx"           // Working directory for intermediate files
    outdir = "${System.getenv('PROJECTS')}/FjellheimLab/martpali/AnnualPerennial/RComPlEx/results"    // Output directory for final results
    
    // Container configuration
    container = "${System.getenv('HOME')}/AnnualPerennial/RComPlEx/RComPlEx.sif"
    
    // Execution parameters
    tissues = ['root', 'leaf']
    test_mode = false
    help = false
}

// Nextflow work directory for temporary execution files (large storage needed)
workDir = "${System.getenv('PROJECTS')}/FjellheimLab/martpali/AnnualPerennial/RComPlEx/work"

// Container configuration (OPTIMIZED for Orion HPC)
singularity {
    enabled = true
    autoMounts = true
    cacheDir = '/local/genome/packages/singularity/CACHE/nextflow'
    // OPTIMIZED: Bind mounts for better I/O and reduce overlay filesystem overhead
    // Scripts: $HOME (small storage) - bind /net/fs-2 and /mnt/users for alternate path access
    // Data/Work: $PROJECTS=/mnt/project (large storage) - bind for work/output directories
    // Note: Use \$ to pass variables to shell for runtime expansion
    runOptions = '--bind $TMPDIR:/tmp --bind /net/fs-2:/net/fs-2 --bind /mnt/users:/mnt/users --bind $PROJECTS:$PROJECTS --no-home --containall'
}

// Disable unused container runtimes
conda {
    enabled = false
}

docker {
    enabled = false
}

// Execution profiles
profiles {

    // Default: SLURM + Apptainer container (all software from SIF)
    slurm {
        process {
            executor = 'slurm'
            queue = 'orion'
            // Standard SLURM options (--gid not available to non-root users)
            clusterOptions = '--account=nmbu --qos=normal'
            scratch = true  // Use local scratch for I/O performance
            
            // OPTIMIZED: File staging for better I/O performance
            stageInMode = 'copy'   // Copy files to work dir (safer for parallel access)
            stageOutMode = 'move'  // Move outputs (faster than copy)

            // Load Apptainer on compute nodes and set group permissions
            beforeScript = '''
                module load singularity
                # Ensure new files inherit group fjellheimlab via SGID
                umask 002
            '''
            
            // FIX GROUP OWNERSHIP: Run after task completes to ensure files are group-owned
            // This is cache-safe as afterScript runs after task hash is computed
            afterScript = '''
                chgrp -R fjellheimlab . 2>/dev/null || true
                chmod -R g+rwX . 2>/dev/null || true
            '''

            // Use container for all processes
            container = params.container

            // Resource allocation by labels (OPTIMIZED for Orion HPC)
            withLabel: 'low_mem' {
                cpus = 2
                memory = '8 GB'
                time = '30m'
                maxForks = 30  // Increased from 20
            }
            
            withLabel: 'medium_mem' {
                cpus = 4
                memory = '16 GB'
                time = '2h'
                maxForks = 15  // Increased from 10
            }
            
            withLabel: 'high_mem' {
                cpus = 2
                memory = '500 GB'
                time = '24h'
                maxForks = 8  // Increased from 5
            }
            
            withLabel: 'very_high_mem' {
                cpus = 12
                memory = '280 GB'
                time = '2d'
                maxForks = 4  // Increased from 2
            }

            // Per-process resource requirements
            withName: PREPARE_PAIR {
                cpus = 2
                memory = '8 GB'
                time = '15m'
                maxForks = 20
            }

            withName: RCOMPLEX_01_LOAD_FILTER {
                cpus = 2
                memory = '8 GB'
                time = '15m'
                maxForks = 20
            }

            withName: RCOMPLEX_02_COMPUTE_SPECIES_NETWORKS {
                // Per-species network computation (formerly per-pair)
                cpus = { task.attempt == 1 ? 24 : 36 }
                memory = { task.attempt == 1 ? '800 GB' : task.attempt == 2 ? '400 GB' : '800 GB' }
                time = '144h'
                maxForks = 13  // Max 13 species per tissue
                errorStrategy = { task.exitStatus in [1, 137, 140] ? 'retry' : 'terminate' }
                maxRetries = 3
            }

            withName: RCOMPLEX_03_LOAD_AND_FILTER_NETWORKS {
                // Lightweight filtering step
                cpus = 2
                memory = '300 GB'
                time = '2h'
                maxForks = 20
            }

            withName: RCOMPLEX_04_NETWORK_COMPARISON {
                // Network comparison (formerly RCOMPLEX_03)
                cpus = { task.attempt == 1 ? 12 : 24 }
                memory = { task.attempt == 1 ? '200 GB' : task.attempt == 2 ? '400 GB' : '600 GB' }
                time = '72h'
                maxForks = 10
                errorStrategy = { task.exitStatus in [1, 137, 140] ? 'retry' : 'terminate' }
                maxRetries = 3
            }

            withName: RCOMPLEX_04_NETWORK_COMPARISON_UNSIGNED {
                // Unsigned comparison (formerly RCOMPLEX_03_UNSIGNED)
                cpus = { task.attempt == 1 ? 12 : 24 }
                memory = { task.attempt == 1 ? '200 GB' : task.attempt == 2 ? '400 GB' : '600 GB' }
                time = '72h'
                maxForks = 10
                errorStrategy = { task.exitStatus in [1, 137, 140] ? 'retry' : 'terminate' }
                maxRetries = 3
            }

            withName: FIND_CLIQUES_UNSIGNED {
                // Same as FIND_CLIQUES
                cpus = 12
                memory = '300 GB'
                time = '48h'
            }

            withName: POLARITY_DIVERGENCE {
                cpus = 4
                memory = { task.attempt == 1 ? '128 GB' : task.attempt == 2 ? '300 GB' : '400 GB' }
                time = '72h'
                errorStrategy = { task.exitStatus in [1, 137, 140] ? 'retry' : 'terminate' }
                maxRetries = 3
            }

            withName: RCOMPLEX_05_SUMMARY_STATS {
                cpus = 4
                memory = '16 GB'
                time = '24h'
                maxForks = 10
            }

            withName: FIND_CLIQUES {
                cpus = 12
                memory = '280 GB'
                time = '48d'
            }
        }

        executor {
            name = 'slurm'
            // OPTIMIZED: Increased queue size for better job throughput
            queueSize = 200
            // OPTIMIZED: More aggressive submission for faster job dispatch
            submitRateLimit = '50/1min'
            pollInterval = '30 sec'
            // OPTIMIZED: Job cleanup and error handling
            exitReadTimeout = '300 sec'
            queueStatInterval = '1 min'
        }
    }

    // Local development (uses container)
    standard {
        process {
            executor = 'local'
            container = params.container
            cpus = 2
            memory = '8 GB'
        }
    }

    // Test profile: limited pairs for quick testing
    test {
        params.test_mode = true
        params.tissues = ['root']

        process {
            executor = 'slurm'
            queue = 'orion'
            beforeScript = 'module load singularity'
            container = params.container

            withName: PREPARE_PAIR {
                maxForks = 2
            }
        }
    }

    // Debug profile: verbose output
    debug {
        process {
            beforeScript = 'echo "Task: $task.name"; echo "Work dir: $task.workDir"; module load singularity'
        }
        cleanup = false
    }
}

// Execution report options
timeline {
    enabled = true
    file = "${params.outdir}/timeline.html"
    overwrite = true
}

report {
    enabled = true
    file = "${params.outdir}/report.html"
    overwrite = true
}

trace {
    enabled = true
    file = "${params.outdir}/trace.txt"
    overwrite = true
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,submit,start,complete,duration,realtime,cpus,memory,rss,vmem,peak_rss,peak_vmem'
}

dag {
    enabled = true
    file = "${params.outdir}/dag.svg"
    overwrite = true
}

// Logging
log {
    dateFormat = 'yyyy-MM-dd HH:mm:ss'
}

// Work directory cleanup
cleanup = false  // Set to true to remove work directories after completion

// Resource limits
process {
    // Enable aggressive caching for smart restarts
    cache = 'lenient'

    // Default error strategy (retry OOM and common transient errors)
    errorStrategy = { task.exitStatus in [143,137,104,134,139] ? 'retry' : 'finish' }
    maxRetries = 1
}

// Maximum resource defaults (must be defined before process scope)
params.max_memory = 1500.GB  // Match Orion node capacity
params.max_cpus = 384         // Match Orion node capacity
params.max_time = 30.d
